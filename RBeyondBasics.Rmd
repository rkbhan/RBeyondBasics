---
title: 'R: Beyond the Basics'
author: "Rongkui Han"
date: "9/10/2018"
output: html_document
---
 
Sample code: https://github.com/dsidavis/RBeyondBasics/blob/master/DH/orig.R    

Problems with this code:   
- line 3: don't do this ever. Don't erase stuff from other people's environment.  
- line 4: use library() if you can.  
- line 83 & 84: don't have the dataset attached. If you can put it online you can put the url there. Or you can put the dataset into a variable, and only download them if the variable doesn't exist locally:

`if(!exists("sp")) {
 sp = /User/blahblah.csv
}`
  
The better practice is to put this data into a package. This is how to do it in *command line*:   
`mkdir r.package`  
`cd r.package`  
`vim DESCRIPTION` #ALL CAPS. All packages need a description file. You can write a description by copying another description file.  
`mkdir inst`   
`cd inst`  
`mkdir mydata`
`cd mydata`
`vim d.csv` #you can look at the data... 2x2
Now you go back into the directory that is the name of your package (r.package). And you do this: 
`R CMD INSTALL .`
Now this package is installed.  

```{r}
library('r.package')
system.file('mydata/d.csv', package = 'r.package')
```

`R CMD build bob`? # didn't follow cuz I didn't build my package right.  

How do you share this package tho?  

(You can have an .RBuildignore file where you list the names of all the files you want the `R CMD build` command to ignore. )

How do you read in the data from your package tho?  
```{r}
data(d) #didn't work. 
```

Back to command line:   
`cd /Users/rongkui/Desktop/Lab/Admin/RBeyondBasics/RBeyondBasics/r.package/`
`mkdir data`

Back to R:  
```{r}
dat = read.csv(file = "r.package/inst/mydata/d.csv")
dat
save(dat, file = "r.package/data/dat.rda")
```

Command line:  
`R CMD INSTALL .`

R:  
```{r}
library(bob)
data(dat)
dat
```
Now you have a package with a readable dataset in it. So basically an R package is a directory (names after the package itself), containing a DESCRIPTION file (that follows specific pattern), an inst dirsctory, and other directories that contain your data and code.   

- Add dependent packages into DESCRIPTION file. Add a line like this:
Depends: tidyvers, dplyr, ggplot2   

- Add small datasets to your package as examples.  

Now we make an "R" directory directly underneath the r.package directory that will contain all the stripts. Clone the orig.R script from the git repository and put it in there.    
Now create a file called NAMESPACE underneath the main directory that reads:
`export(tidy)`
You want to export any functions in any of the R codes that you wrote that you want your users to see.   

```{r}
library(r.package)
r.package::simple
ls(2) #this is problematic. I cannot find any of the functions. 
ls() #only found the dataset.  
```

Didn't work. SMH. So I'm going to manually type in the tidy function so we could work with it later. 

```{r}
tidy <- function(catch, drifts){
  
  # initialize variables
  # max and min depth
  min_depth <- min(c(catch$start_depth, catch$end_depth))
  max_depth <- max(c(catch$start_depth, catch$end_depth))
  # max and min size
  max_size <- max(catch$size)
  min_size <- min(catch$size)
  # three letter site names
  sites <- unique(x = substr(drifts$IDCell.per.Trip, 0, 3))
  
  # initialize marticies
  # total CPUE
  total_CPUE <- matrix(0L, nrow = (max_depth - min_depth + 1), ncol = (max_size - min_size + 1))
  colnames(total_CPUE) <- c(dQuote(min_size:max_size))
  rownames(total_CPUE) <- c(dQuote(min_depth:max_depth))
  # site-specific CPUE
  CPUEperSite <- matrix(0L, nrow = (length(sites)), ncol = (max_size - min_size + 2))
  colnames(CPUEperSite) <- c("Site", dQuote(min_size:max_size))
  CPUEperSite[,1] <- sites
  # construct total effort for each depth
  # effort <- effort_function()
  
  # tidy dataframe (site, size, depth, CPUE)
  tidy_data <- data.frame(row.names = c("site", "size", "depth", "catch"))
  
  
  #loops
  # calculate CPUE per site
  for(x in 1:nrow(catch)){ #this will run through every row in the BLU_only_depths matrix
    # x counts the row of the BLU_only_depths matrix that is being currently worked on
    # determine if the start or end depth is deeper and save the deeper value
    # pmax stands for parallel maximum, which compares two columns and finds the maximum value
    deeper <- pmax(catch$start_depth[x], catch$end_depth[x])
    # determine if the start or end depth is shallower and save the shallower depth
    shallower <-pmin(catch$start_depth[x], catch$end_depth[x])
    # new dataframe to collect all the data in this iteration of the loop
    this_fish <- data.frame(site = c(rep(substr(catch$IDCell.per.Trip[x], 0, 3), times = (deeper - shallower +1))), size = c(rep(catch$size[x], times = (deeper - shallower +1))), depth = c(shallower:deeper), catch = c(rep(1, times = (deeper - shallower +1))))
    # Add 1 to the column associated with the fish's length and all the rows in the range of depths fished for that drift
    tidy_data <- rbind(tidy_data, this_fish)
    # time keeper
    print(x)
  }

#  return(tidy_data)

#orig = tidy_data    
  # tallies the number of columns that match the 'group_by' call and collapses them down to one row
  tidy_data <- tidy_data %>%
    group_by(site, size, depth) %>%
    tally()
  # matches the depth column in 'effort' to the depth column in 'tidy_data' and inputs the associated effort value as a new column in tidy-data
  tidy_data$effort <- effort[match(tidy_data$depth, effort$depth), 2]
  
  
  # Trying to control for the posibility that one site contributed the majority of the CPUE to the grand total, thereby masking the data from the other sites.
  # The way that it is being calculated here makes the assumption that, since they fished the same number of tansects, they fished for the same amount of time at each location
  # An ANOVA confirms that total drift times are not significantly different across sites
  # After calculating CPUE, total CPUE for each site is added up.
  # Then CPUE is divide by the site total
  
  # add a row that calculates CPUE based on total effort
  tidy_data <- mutate(tidy_data, CPUE = n/effort)
  # new variable that calculates the total CPUE per site
  CPUE_by_site <- tidy_data %>%
    group_by(site) %>%
    summarize(site_total_CPUE = sum(CPUE))
  # merge CPUE_by_site with tidy_data by matching the name 'site'
  tidy_data <- merge(tidy_data, CPUE_by_site, by='site')
  tidy_data <- mutate(tidy_data, normalized_CPUE = CPUE/site_total_CPUE)
  
  
  return(tidy_data)
}
```


---Upgrading your package---    

- Change "Depends:" into "Imports:" in the DESCRIPTION file. Always use Import unless you have a very good reason.   

- Add "import(dplyr)" to NAMESPACE file.   

---Some code review---   

Looking at the orig.R code.  

line 10: concatinating two vectors with c(,) for the min() function. But if you look at the min() function, it takes "..." as an input, meaning it will take anything and concatinate them in the backstage. So you don't have to do the concatination.    

line 36: for loops are bad. sapply and lapply isn't much differet.   
line 36: if the "catch" dataframe is empty, the nrow() will be 0. The loop will run twice with i = 1 and 0 and stop, and you won't get an error message.     
line 36-80: how do we make this faster? 
- line 40: pmax -> max
- line 40 

line 48: time keeper with print(x) is slow. we do this:  

`function(catch, drift, verbose = TRUE) {
  blahblah
  if (verbose && x%% 100 ==) print (x)
}`

line 46: rbind is bad. Everytime you rbind you create two dataframe that are almost identical. You could pre-allocate the space and fill in the space. But in this case we do not know how much space we need at the end.

- first, keep each i an item on a list (of undetermined length):
`results = list()`
- second, get the rbind out of the loop. towards the end. So it will rbind everything in one go instead of once per iteration. This is much more efficient. 
`tidy_data = rbind(results)`
- now go back to the list. If you shove each element into the list of undetermined length, you create the same problem with rbind. So you still have to pre-allocate space onto the list. 
`results = vector("list", nrow(catch))` (it is impossible to create a list of certain length with the function list().)
- then you could do this:  
`do.call(rbind, results)`. 

Here is how do.call works:  
```{r}
f = function(a,b) a+b
f(1,10)
do.call(f, list(1,10))
```

How do you find global variables in a function:  
```{r}
codetools::findGlobals(tidy, FALSE) #the false separates the functions from the variables. 
```

Here codetools found 7 global variables, but some of them are false positives as a result of tidyerse, because tidyverse takes non-standard expression for R. The true positive here is "effort". Duncan recommands people using the codetools::findGlobals() function everytime they write a function.  

- The key trick here is rep(vectorA, vectorB), where vectors A and B have the same length. say A = c("A","B","C","D") and B = c(2,5,3,6), rep() will repeat A twice, B five times and so on. 

*VECTORIZATION* is key to speed things up. You can speed things up by the thousands.  

```{r}
getwd()
```


9/12/18

- Today we are looking at Machaela's code.   
1. always add `print(i)` in your loops so you know it is running.   
2. hard coded numbers = questionable reusability  
3. perfect code = code that works. Write functional stuff and upgrade it afterwards.  
4. 


